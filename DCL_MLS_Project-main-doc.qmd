---
title: "Causal Reasoning of LLMs"
format: html 
editor: visual
lang: de
author: 
  - name: "Lucas Mandelik"
    affiliation: "HWI Hamburg"
    email: "lucas.mandelik@studium.uni-hamburg.de"
  - name: "Alexander Lorenz"
    affiliation: "HWI Hamburg"
    email: "alexander.lorenz@studium.uni-hamburg.de"
  - name: "Jannik Svenson"
    affiliation: "HWI Hamburg"
    email: "jannik.svenson@studium.uni-hamburg.de"
date: "30. Juni 2024"
---

<!-- format: pptx -->

AKTUALISIERUNGSDATUM <br> 26. Juni 2024

## Inhaltsverzeichnis

1.  [Einleitung](#einleitung)
2.  [Grundlagen](#grundlagen)
3.  [Parrots Paradox](#parrotsparadox)
4.  [Beitrag](#beitrag)
5.  [Schlussbetrachtung](#schlussbetrachtung)
6.  [Referenzen](#referenzen)

## Einleitung {#einleitung}

Eine detailliertere Beschreibung des Projekts, seiner Ziele und Funktionen.

## Grundlagen {#grundlagen}

Zuerst kann einmal festgehalten werden, dass wir uns im folgenden hauptsächlich mit dem nachfolgenden oft debattierten Paper: XXXXX beschäftigen.

## ParrotsParadox {#parrotsparadox}

Einige Nutzer von LLMs haben bemerkt, dass LLMs nicht zwingend immer Kausalität erkennen können und sie z.B. auf einfache kausale Fragen falsch antworten. In unserem Repository haben wir einige Fälle gesammelt, die zweifelhafte kausale Fähigkeiten von Chat GPT aufzeigen. Außerdem haben wir auch aus der Litertur bekannte Fragen, bei denen LLMs schonmal gescheitert sind, überprüft. Generell kann ChatGPT 4.0 und auch bedingt Chat GPT 3.5 Turbo diese Fragen gut beantworten, was aber nicht an dem kausalen Verständnis liegen muss, da diese Cases auch anders beantwortet werden könnten. Beispielsweise ist es denkbar, dass OpenAI diese Problem erkannt hat und entschieden hat bekannte Fälle und Fragen, die Sie von Nutzern mitbekommen, direkt z.B. durch "hardcoden" entfernen könnten. Außerdem könnten solche Fälle auch in den Trainingsdaten insbesondere von neueren LLMs vorhanden sein.

## Beitrag {#beitrag}
Wir haben uns dann weiterhin damit beschäftigt, wie die beiden Versionen von UHHGPT, nämlich ChatGPT 3.5 Turbo und ChatGPT 4 omni, die Bechmarks erfüllt, die das Paper vorgeschlagen hat.  

### Paper - Beispielfragen
Dazu haben wir zunächst die Fälle ausprobiert, die in dem Paper vorgestellt wurden. Dabei werden ChatGPT immer Sätze vorgelegt und darf immer nur mit "True" oder "False" antworten. Den ersten Fall und die Antwort von ChatGPT 3.5 Turbo kann man im folgenden Bild sehen:
<img src="LLM Fails\Edge Cases\Fail heights.png" align="center" width = 100% />
Da Bob Höhenangst hat, wäre die logische Konsequenz daraus, dass er bei der Wahl zwischen dem Riesenrad und dem Karussell das Riesenrad vermeiden würde. ChatGPT 3.5 Turbo ist in diesem Fall nicht dazu in der Lage gewesen, diesen kausalen Zusammenhang zwischen Höhenangst und dem Riesenrad zu ziehen, dass Bob durch das Riesenrad in die Höhe transportiert würde, was seine Höhenangst auslösen würde und er aus genau diesem Grund das Karussell dem Riesenrad vorziehen würde. ChatGPT 4 omni hingegen hat diesen Fall richtig gelöst.  
Tatsächlich sind wir dabei auch auf einen interessanten Fehler gestoßen. ChatGPT 4 omni hat bislang sehr gute Ergebnisse geliefert und hat bei keiner Frage, die wir ausgetestet haben, falsche Ergebnisse geliefert. So auch zuerst bei diesem Fall, der ebenfalls aus dem untersuchten Paper stammt:
<img src="LLM Fails\Academic Paper Cases\fish correct.png" align="center" width = 100% />
Da Sammy mit 10 Fischen rechnen würde, wäre er folglich bei 40 Fischen sehr erfreut und bei nur 5 Fischen vermutlich enttäuscht. Das hat ChatGPT 4 omni ohne Probleme hinbekommen. Interessanterweise ist uns aufgefallen, dass, sobald man den Namen von Sammy klein schreibt, ChatGPT 4 omni nicht mehr in der Lage ist, dies richtig zu lösen, wie man hier sehen kann:
<img src="LLM Fails\Academic Paper Cases\4.0 Fail.png" align="center" width = 100% />
Hier ist es also eventuell der Fall, dass ChatGPT nicht mehr in der Lage ist, zu identifizieren, dass es sich bei Sammy um eine Person handelt, weshalb hier dann kein logischer Zusammenhang erkannt werden kann.

### Eigene Ideen und bekannte Fälle
Desweiteren haben wir andere Fälle ausgetestet, die in der Vergangenheit aufgefallen sind, da ChatGPT sie nicht korrekt beantworten konnte und haben auch eigene Ideen ausgetestet, um zu prüfen, ob ChatGPT hier in der Lage dazu ist, kausale Zusammenhänge zu erkennen.  
Im ersten Beispiel geht es um das Thema Fußball:
<img src="LLM Fails\Edge Cases\3.5 Fail Fussball.png" align="center" width = 100% />
In diesem Fall ist ChatGPT 3.5 der Logik gefolgt, dass es hier eine einfache transitive Eigenschaft gibt, dass wenn Team A Team B besiegt und Team C Team A besiegt, Team C ja folglich besser sein müsste als Team B und somit auch Team B besiegen müsste. Dies ist allerdings beim Thema Fußball keine gegebene Eigenschaft, da die Teams verschiedene Spieltaktiken nutzen, die bei manchen Teams ein Schwachpunkt darstellen, von anderen Spieltaktiken jedoch ausgekontert werden. Das kann man z.B. auch gut in der EM betrachten. Deutschland hat z.B. 5:1 gegen Schottland gespiel. Da Schottland 1:1 gegen die Schweiz gespielt hat, müsste aus der Logik von ChatGPT 3.5 Deutschland auch deutlich gegen die Schweiz gewinnen, hier kam jedoch nur ein knappes Unentschieden zustande. Auch hier hat ChatGPT 4 omni jedoch die Nase vorne und konnte auch diese Frage richtig beantworten.  
Ein weiteres Beispiel kann hier gesehen werden:
<img src="LLM Fails\Edge Cases\Fail Box.png" align="center" width = 100% />
In der Fragestellung wird formuliert, dass es in einer blauen Box ein Apfel und eine rote Box mit einem Deckel gibt. Einem Menschen würde dabei sofort auffallen, dass folglich der Apfel einfach aus der blauen Box entnommen werden kann. ChatGPT 3.5 Turbo hingegen hat es hier jedoch nicht geschafft, diesen Zusammenhang zu erkennen - wir vermuten, dass das der Fall ist, da so genau beschrieben wurde, dass die rote Box einen Deckel hat, und dieser roten Box deswegen ein zu großer Wert für die Beantwortung der Frage beigemessen wurde.  
Auch hier konnte ChatGPT 4 omni jedoch korrekt antworten und hat darauf hingewiesen, dass die rote Box gar keine Rolle spielen würde, wenn man an den Apfel gelangen möchte. 

## Schlussbetrachtung {#schlussbetrachtung}

eventuell elegant abschließen?

## Referenzen {#referenzen}

[Linying Yang, Vik Shirvaikar, Oscar Clivio & Fabian Falck](https://openreview.net/forum?id=mRwgczYZFJ).

## Anhang

### Spaß über LLMs vor GAI

Nachfolgende Späße sind nicht despektierlich gegenüber LLMs oder GAI gemeint, sondern sollen helfen diese voranzubringen.

[ChatGPT Failures](https://github.com/giuven95/chatgpt-failures.git)

# Testing Ground

In diesem Dokument wird ein LaTeX-Dokument in Quarto eingebunden.

```{verbatim}
\documentclass{article}
\usepackage{lipsum} % Beispieltextpaket

\title{Beispiel LaTeX Dokument}
\author{Autor Name}
\date{\today}

\begin{document}

\maketitle

\section{Einleitung}
\lipsum[1-2] % Beispieltext

\section{Hauptteil}
\lipsum[3-4] % Beispieltext

\end{document}
```

{include, file="latex_files/test.tex", raw=true}

```{=tex}
\begin{tabular}{|l|l|}\hline
Age & Frequency \\ \hline
18--25  & 15 \\
26--35  & 33 \\
36--45  & 22 \\ \hline
\end{tabular}
```
# demo stuff

\vspace{30pt}

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
1 + 1
```

You can add options to executable code like this

```{r}
#| echo: false
2 * 2
```

The `echo: false` option disables the printing of code (only output is displayed).

## Include figures

You can also include some figures, for example

![](figures/logo.png){width="50%"}

## Load data

You can load data using relative paths. Here we load data from the subdirectory `data`

```{r}
load("data/experiment_data_counterfactual.rda")
head(df)
```

## Load R code

```{r}
# Load the R code in the file `R/my_function.R"
source("R/my_function.R")

set.seed(1234)
x <- draw(10)
print(x)
```

<!-- Text -->
